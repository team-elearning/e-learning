import pandas as pd
import numpy as np
from django.core.cache import cache
from django.db import transaction
from django.db.models import Count, Avg, Max, F, Q
from django.utils import timezone
from datetime import timedelta
from typing import Optional

# Import Models
from content.models import Enrollment, Course, Lesson, Quiz
from progress.models import QuizAttempt
from analytics.models import UserActivityLog, StudentSnapshot
from analytics.domains.student_profile_domain import StudentRiskProfile



# ==========================================
# ANALYZE
# ==========================================

def analyze_course_health_bulk(course_id: str):
    """
    Ph√¢n t√≠ch r·ªßi ro cho to√†n b·ªô h·ªçc vi√™n trong kh√≥a h·ªçc.
    N√™n ch·∫°y qua Celery/Cronjob (v√≠ d·ª•: m·ªói ƒë√™m ho·∫∑c m·ªói 6 ti·∫øng).
    """
    # 1. PREPARE DATA (L·∫•y ID tham chi·∫øu)
    # ---------------------------------------------------------
    # L·∫•y danh s√°ch Lesson ID v√† Quiz ID thu·ªôc kh√≥a h·ªçc ƒë·ªÉ filter log
    lesson_ids = list(Lesson.objects.filter(module__course_id=course_id).values_list('id', flat=True))
    
    # L·∫•y Quiz ID th√¥ng qua ContentBlock (nh∆∞ b·∫°n m√¥ t·∫£: ContentBlock -> Lesson -> Module -> Course)
    quiz_ids = list(Quiz.objects.filter(
        content_blocks__lesson__module__course_id=course_id
    ).values_list('id', flat=True))
    
    lesson_ids_str = [str(uid) for uid in lesson_ids]
    quiz_ids_str = [str(uid) for uid in quiz_ids]
    course_id_str = str(course_id)

    # 2. FETCH DATA (Ch·ªâ 3 Queries l·ªõn thay v√¨ N+1)
    # ---------------------------------------------------------
    
    # Q1: L·∫•y danh s√°ch h·ªçc vi√™n (Base DataFrame)
    # Ch·ªâ l·∫•y fields c·∫ßn thi·∫øt ƒë·ªÉ ti·∫øt ki·ªám RAM
    enrollments = Enrollment.objects.filter(course_id=course_id).values(
        'user_id', 'percent_completed'
    )
    if not enrollments: return # Kh√≥a h·ªçc v·∫Øng tanh
    
    df_students = pd.DataFrame(list(enrollments))
    df_students.set_index('user_id', inplace=True) # Index l√† User ID

    # Q2: L·∫•y Logs t∆∞∆°ng t√°c (30 ng√†y qua)
    thirty_days_ago = timezone.now() - timedelta(days=30)
    logs = UserActivityLog.objects.filter(
        timestamp__gte=thirty_days_ago,
        # Filter Log thu·ªôc v·ªÅ Course/Lesson/Quiz c·ªßa kh√≥a n√†y
    ).filter(
        Q(entity_type='course', entity_id=course_id_str) |
        Q(entity_type='lesson', entity_id__in=lesson_ids_str) |
        Q(entity_type='quiz', entity_id__in=quiz_ids_str)
    ).values('user_id', 'action', 'timestamp')

    df_logs = pd.DataFrame(list(logs))
    
    # Q3: L·∫•y ƒëi·ªÉm Quiz (L·∫•y ƒëi·ªÉm trung b√¨nh c·ªßa c√°c b√†i ƒë√£ ch·∫•m)
    # Filter theo quiz_ids thu·ªôc kh√≥a h·ªçc
    attempts = QuizAttempt.objects.filter(
        quiz_id__in=quiz_ids,
        status='graded'
    ).values('user_id', 'score', 'max_score')
    
    df_quizzes = pd.DataFrame(list(attempts))

    # 3. MATRIX CALCULATION (Pandas Magic)
    # ---------------------------------------------------------

    # A. T√çNH ENGAGEMENT (T·ª´ df_logs)
    if not df_logs.empty:
        # Convert timestamp
        df_logs['timestamp'] = pd.to_datetime(df_logs['timestamp'])
        now = pd.Timestamp.now(tz='utc') # L∆∞u √Ω timezone

        # Group by User ƒë·ªÉ t√≠nh c√°c ch·ªâ s·ªë
        log_stats = df_logs.groupby('user_id').agg(
            last_access=('timestamp', 'max'),
            total_actions=('action', 'count'),
            # ƒê·∫øm s·ªë action quan tr·ªçng (Lambda function h∆°i ch·∫≠m, nh∆∞ng ok v·ªõi data v·ª´a ph·∫£i)
            # C√°ch nhanh h∆°n: T·∫°o c·ªôt 'is_high_value' tr∆∞·ªõc r·ªìi sum
            high_value_actions=('action', lambda x: x.isin(['QUIZ_SUBMIT', 'VIDEO_COMPLETE']).sum())
        )

        # T√≠nh days_inactive
        log_stats['days_inactive'] = (now - log_stats['last_access']).dt.days

        # T√≠nh Engagement Score (Vector h√≥a c√¥ng th·ª©c)
        # Score = (Total + HighVal * 4) / 10
        log_stats['eng_score_raw'] = (log_stats['total_actions'] + log_stats['high_value_actions'] * 4) / 10
        
        # Clip score v·ªÅ 10
        log_stats['eng_score'] = log_stats['eng_score_raw'].clip(upper=10.0)
        
        # Ph·∫°t ƒëi·ªÉm n·∫øu inactive (Vector h√≥a logic if/else)
        # np.where(condition, true_val, false_val)
        log_stats['eng_score'] = np.where(log_stats['days_inactive'] > 7, log_stats['eng_score'] * 0.5, log_stats['eng_score'])
        log_stats['eng_score'] = np.where(log_stats['days_inactive'] > 14, 0.0, log_stats['eng_score'])
        
        # Merge v√†o b·∫£ng h·ªçc sinh (Left Join - User n√†o ko c√≥ log th√¨ NaN)
        df_final = df_students.join(log_stats, how='left')
    else:
        # Tr∆∞·ªùng h·ª£p kh√¥ng c√≥ log n√†o
        df_final = df_students.copy()
        df_final['eng_score'] = 0.0
        df_final['days_inactive'] = 30 # Default

    # B. T√çNH PERFORMANCE (T·ª´ df_quizzes & df_students)
    if not df_quizzes.empty:
        # Chu·∫©n h√≥a ƒëi·ªÉm quiz v·ªÅ thang 10 (score / max_score * 10)
        df_quizzes['normalized_score'] = (df_quizzes['score'] / df_quizzes['max_score']) * 10
        # T√≠nh trung b√¨nh ƒëi·ªÉm quiz m·ªói user
        quiz_avg = df_quizzes.groupby('user_id')['normalized_score'].mean()
        
        df_final = df_final.join(quiz_avg.rename('avg_quiz_score'), how='left')
    else:
        df_final['avg_quiz_score'] = 0.0

    # Fill NaN b·∫±ng 0 (cho user kh√¥ng l√†m quiz ho·∫∑c ko c√≥ log)
    df_final.fillna(0, inplace=True)
    
    # T√≠nh Performance Score t·ªïng h·ª£p
    # Perf = (AvgQuiz * 0.6) + ((PercentCompleted / 10) * 0.4)
    df_final['perf_score'] = (df_final['avg_quiz_score'] * 0.6) + ((df_final['percent_completed'] / 10) * 0.4)
    df_final['perf_score'] = df_final['perf_score'].round(2)
    
    # 4. RISK ASSESSMENT (Vectorized Logic)
    # ---------------------------------------------------------
    
    # T·∫°o c·ªôt Risk Level m·∫∑c ƒë·ªãnh
    df_final['risk_level'] = 'low'
    df_final['message'] = 'Duy tr√¨ t·ªët'

    # Apply logic ph√¢n lo·∫°i (D√πng np.select gi·ªëng nh∆∞ SQL Case When)
    # ƒêi·ªÅu ki·ªán
    cond_dropout = df_final['days_inactive'] > 21
    cond_at_risk = (df_final['days_inactive'] > 7) | (df_final['eng_score'] < 3.0)
    cond_struggling = (df_final['eng_score'] >= 6.0) & (df_final['perf_score'] < 5.0)
    cond_disengaged = (df_final['perf_score'] >= 8.0) & (df_final['eng_score'] < 5.0)

    # Gi√° tr·ªã t∆∞∆°ng ·ª©ng
    choices_risk = ['critical', 'high', 'medium', 'medium']
    choices_msg = [
        'V·∫Øng m·∫∑t qu√° l√¢u (Dropout?)',
        '√çt t∆∞∆°ng t√°c (At Risk)',
        'G·∫∑p kh√≥ khƒÉn ki·∫øn th·ª©c (Struggling)',
        'H·ªçc t√†i t·ª≠ (Disengaged)'
    ]

    # √Åp d·ª•ng
    df_final['risk_level'] = np.select(
        [cond_dropout, cond_at_risk, cond_struggling, cond_disengaged], 
        choices_risk, 
        default='low'
    )
    df_final['message'] = np.select(
        [cond_dropout, cond_at_risk, cond_struggling, cond_disengaged], 
        choices_msg, 
        default='K·∫øt qu·∫£ t·ªët'
    )

    # 5. BULK SAVE TO DB
    # ---------------------------------------------------------
    save_snapshots(course_id, df_final)


# ==========================================
# SNAPSHOT
# ==========================================

@transaction.atomic
def save_snapshots(self, course_id, df_result):
    """
    L∆∞u k·∫øt qu·∫£ ph√¢n t√≠ch.
    LOGIC M·ªöI: Gi·ªØ l·∫°i l·ªãch s·ª≠ (Append-only), kh√¥ng x√≥a c√°i c≈©.
    ƒê·ªÉ tr√°nh spam DB, ta c√≥ th·ªÉ check xem h√¥m nay ƒë√£ ch·∫°y ch∆∞a (Optional).
    """
    
    # [OPTIONAL]: X√≥a b·∫£n ghi C·ª¶A NG√ÄY H√îM NAY ƒë·ªÉ tr√°nh duplicate n·∫øu ch·∫°y job nhi·ªÅu l·∫ßn trong ng√†y
    # Gi·ªØ l·∫°i b·∫£n ghi c·ªßa ng√†y h√¥m qua, tu·∫ßn tr∆∞·ªõc...
    today_start = timezone.now().replace(hour=0, minute=0, second=0, microsecond=0)
    
    deleted_count, _ = StudentSnapshot.objects.filter(
        course_id=course_id, 
        created_at__gte=today_start # Ch·ªâ x√≥a c√°i v·ª´a t·∫°o h√¥m nay (n·∫øu ch·∫°y l·∫°i)
    ).delete()
    
    if deleted_count > 0:
        print(f"üîÑ Re-running analytics for today. Deleted {deleted_count} partial records.")

    # Chu·∫©n b·ªã list object
    snapshots = []
    for user_id, row in df_result.iterrows():
        snapshots.append(StudentSnapshot(
            user_id=user_id,
            course_id=course_id,
            
            # C√°c ch·ªâ s·ªë
            engagement_score=row['eng_score'],
            performance_score=row['perf_score'],
            days_inactive=int(row['days_inactive']),
            
            # K·∫øt lu·∫≠n
            risk_level=row['risk_level'],
            ai_message=row['message'],
            
            # [QUAN TR·ªåNG] L∆∞u √Ω: created_at s·∫Ω t·ª± ƒë·ªông l·∫•y gi·ªù hi·ªán t·∫°i (auto_now_add)
        ))
    
    # Bulk Create (Insert 1 c·ª•c)
    StudentSnapshot.objects.bulk_create(snapshots, batch_size=500)
    print(f"‚úÖ Saved history for {len(snapshots)} students in Course {course_id}")

    # [QUAN TR·ªåNG] INVALIDATE CACHE
    # Khi ƒë√£ t√≠nh to√°n xong snapshot m·ªõi, d·ªØ li·ªáu tr√™n Dashboard c≈© ƒë√£ b·ªã l·ªói th·ªùi.
    # Ta x√≥a key cache ƒëi ƒë·ªÉ l·∫ßn t·ªõi Gi·∫£ng vi√™n F5, h·ªá th·ªëng s·∫Ω t√≠nh l·∫°i d·ªØ li·ªáu m·ªõi nh·∫•t.
    cache_key = f"course_pulse_{course_id}"
    cache.delete(cache_key)
    
    print(f"‚ôªÔ∏è Cache invalidated for {cache_key}")