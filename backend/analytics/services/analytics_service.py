import numpy as np
import pandas as pd
import time
from django.core.cache import cache
from django.core.paginator import Paginator
from django.db import transaction
from django.db.models import Count, Avg, Max, F, Q
from django.utils import timezone
from datetime import timedelta
from typing import Optional, List, Union

# Import Models
from content.models import Enrollment, Course, Lesson, Quiz
from progress.models import QuizAttempt
from analytics.models import UserActivityLog, StudentSnapshot, CourseAnalyticsLog
from analytics.domains.analytics_result_domain import AnalyticsJobResultDomain
from analytics.domains.course_health_overview_domain import CourseHealthOverviewDomain
from analytics.domains.risk_distribution_domain import RiskDistributionDomain
from analytics.domains.paginated_student_list_domain import PaginatedStudentListDomain
from analytics.domains.student_risk_info_domain import StudentRiskInfoDomain
from analytics.domains.analytics_log_domain import AnalyticsLogDomain



# --- CONFIGURATION (N√™n ƒë∆∞a v√†o Settings ho·∫∑c DB Config) ---
RISK_CONFIG = {
    'INACTIVE_WARN_DAYS': 7,
    'INACTIVE_CRITICAL_DAYS': 21,
    'LOW_ENGAGEMENT_THRESHOLD': 3.0,
    'LOW_PERFORMANCE_THRESHOLD': 5.0,
    'HIGH_PERFORMANCE_THRESHOLD': 8.0,
}

# ---------------------------------------------------------
# PRIVATE HELPER METHODS 
# ---------------------------------------------------------

def _fetch_enrolled_students(course_id: str) -> pd.DataFrame:
    """L·∫•y danh s√°ch h·ªçc vi√™n v√† % ti·∫øn ƒë·ªô"""
    enrollments = Enrollment.objects.filter(course_id=course_id).values('user_id', 'percent_completed')
    df = pd.DataFrame(list(enrollments))
    if not df.empty:
        df.set_index('user_id', inplace=True)
    return df


def _calculate_engagement_metrics(course_id: str, student_ids: list) -> pd.DataFrame:
    """T√≠nh to√°n ch·ªâ s·ªë t∆∞∆°ng t√°c t·ª´ UserActivityLog"""
    thirty_days_ago = timezone.now() - timedelta(days=30)
    
    # Query Aggregate tr·ª±c ti·∫øp t·ª´ DB (T·ªëi ∆∞u RAM)
    log_stats_qs = UserActivityLog.objects.filter(
        course_id=course_id,
        timestamp__gte=thirty_days_ago,
        user_id__in=student_ids
    ).values('user_id').annotate(
        last_access=Max('timestamp'),
        total_actions=Count('id'),
        high_value_actions=Count('id', filter=Q(action__in=['QUIZ_SUBMIT', 'VIDEO_COMPLETE']))
    )

    if not log_stats_qs:
        # Tr·∫£ v·ªÅ DataFrame r·ªóng nh∆∞ng c√≥ ƒë√∫ng c·ªôt ƒë·ªÉ join kh√¥ng b·ªã l·ªói
        return pd.DataFrame(index=student_ids, columns=['days_inactive', 'eng_score']).fillna(0)

    df = pd.DataFrame(list(log_stats_qs))
    df.set_index('user_id', inplace=True)
    
    # T√≠nh to√°n
    now = pd.Timestamp.now(tz='utc')
    if df['last_access'].dt.tz is None:
            df['last_access'] = df['last_access'].dt.tz_localize('UTC')
            
    df['days_inactive'] = (now - df['last_access']).dt.days
    
    # C√¥ng th·ª©c Engagement Score
    df['eng_score'] = (
        (df['total_actions'] + df['high_value_actions'] * 4) / 10
    ).clip(upper=10.0)
    
    return df[['days_inactive', 'eng_score']]


def _calculate_performance_metrics(course_id: str, student_ids: list) -> pd.DataFrame:
    """T√≠nh to√°n ƒëi·ªÉm s·ªë trung b√¨nh t·ª´ QuizAttempt"""
    # Logic t√≠nh ƒëi·ªÉm normalized
    attempts = QuizAttempt.objects.filter(
        enrollment__course_id=course_id,
        status='graded',
        user_id__in=student_ids
    ).values('user_id', 'score', 'max_score')
    
    if not attempts:
        return pd.DataFrame(index=student_ids, columns=['avg_quiz_score']).fillna(0)

    df = pd.DataFrame(list(attempts))
    
    # Vectorized Normalization (Tr√°nh chia cho 0)
    df['normalized'] = np.where(
        df['max_score'] > 0, 
        (df['score'] / df['max_score']) * 10, 
        0.0
    )
    
    # Group by user ƒë·ªÉ l·∫•y ƒëi·ªÉm trung b√¨nh
    quiz_avg = df.groupby('user_id')['normalized'].mean()
    return quiz_avg.to_frame(name='avg_quiz_score')


def _assess_risk_matrix(self, df: pd.DataFrame) -> pd.DataFrame:
    """√Åp d·ª•ng logic ph√¢n lo·∫°i r·ªßi ro (Risk Matrix)"""
    # 1. T√≠nh Performance Score t·ªïng h·ª£p
    df['perf_score'] = (
        (df['avg_quiz_score'] * 0.6) + 
        ((df['percent_completed'] / 10) * 0.4)
    ).round(2)

    # 2. Ph·∫°t ƒëi·ªÉm Engagement n·∫øu ngh·ªâ qu√° l√¢u
    df['eng_score'] = np.where(
        df['days_inactive'] > RISK_CONFIG['INACTIVE_WARN_DAYS'], 
        df['eng_score'] * 0.5, 
        df['eng_score']
    )
    # N·∫øu ngh·ªâ qu√° 2 tu·∫ßn -> Engagement v·ªÅ 0
    df['eng_score'] = np.where(df['days_inactive'] > 14, 0.0, df['eng_score'])

    # 3. Ph√¢n lo·∫°i (Logic if/else ma tr·∫≠n)
    c_dropout = df['days_inactive'] > RISK_CONFIG['INACTIVE_CRITICAL_DAYS']
    c_at_risk = (df['days_inactive'] > RISK_CONFIG['INACTIVE_WARN_DAYS']) | (df['eng_score'] < RISK_CONFIG['LOW_ENGAGEMENT_THRESHOLD'])
    c_struggling = (df['eng_score'] >= 6.0) & (df['perf_score'] < RISK_CONFIG['LOW_PERFORMANCE_THRESHOLD'])
    c_disengaged = (df['perf_score'] >= RISK_CONFIG['HIGH_PERFORMANCE_THRESHOLD']) & (df['eng_score'] < 5.0)

    df['risk_level'] = np.select(
        [c_dropout, c_at_risk, c_struggling, c_disengaged], 
        ['critical', 'high', 'medium', 'medium'], 
        default='low'
    )

    df['message'] = np.select(
        [c_dropout, c_at_risk, c_struggling, c_disengaged], 
        [
            f"V·∫Øng m·∫∑t > {RISK_CONFIG['INACTIVE_CRITICAL_DAYS']} ng√†y", 
            'T∆∞∆°ng t√°c th·∫•p (C·∫ßn nh·∫Øc nh·ªü)', 
            'ƒêi·ªÉm th·∫•p d√π chƒÉm ch·ªâ (C·∫ßn h·ªó tr·ª£)', 
            'H·ªçc ƒë·ªëi ph√≥/Gi·ªèi nh∆∞ng l∆∞·ªùi'
        ], 
        default='K·∫øt qu·∫£ t·ªët'
    )
    return df


# ==========================================
# ANALYZE
# ==========================================

@transaction.atomic
def analyze_course_health_bulk(course_id: str) -> AnalyticsJobResultDomain:
    start_time = time.time()
    
    # 1. Fetch Students (Base Population)
    df_students = _fetch_enrolled_students(course_id)
    if df_students.empty:
        return AnalyticsJobResultDomain(course_id, 0, 0, 'skipped_empty', 0)
    
    student_ids = df_students.index.tolist()

    # 2. Calculate Engagement (T·ª´ Log)
    df_engagement = _calculate_engagement_metrics(course_id, student_ids)
    
    # 3. Calculate Performance (T·ª´ Quiz)
    df_performance = _calculate_performance_metrics(course_id, student_ids)
    
    # 4. Merge & Final Risk Assessment
    # Join c√°c dataframe l·∫°i v·ªõi nhau
    df_final = df_students.join(df_engagement, how='left')\
                            .join(df_performance, how='left')
    
    # ƒêi·ªÅn 0 cho nh·ªØng user kh√¥ng c√≥ log/quiz
    df_final.fillna(0, inplace=True)
    
    # Ch·∫°y logic ph√¢n lo·∫°i r·ªßi ro
    df_result = _assess_risk_matrix(df_final)

    # 5. Save to DB
    save_snapshots(course_id, df_result)

    execution_time = round(time.time() - start_time, 2)
    return AnalyticsJobResultDomain(
        course_id=str(course_id),
        total_students=len(df_students),
        processed_count=len(df_result),
        status='success',
        execution_time=execution_time
    )


# ==========================================
# SNAPSHOT
# ==========================================

@transaction.atomic
def save_snapshots(self, course_id, df_result):
    """
    L∆∞u k·∫øt qu·∫£ ph√¢n t√≠ch.
    LOGIC M·ªöI: Gi·ªØ l·∫°i l·ªãch s·ª≠ (Append-only), kh√¥ng x√≥a c√°i c≈©.
    ƒê·ªÉ tr√°nh spam DB, ta c√≥ th·ªÉ check xem h√¥m nay ƒë√£ ch·∫°y ch∆∞a (Optional).
    """
    
    # [OPTIONAL]: X√≥a b·∫£n ghi C·ª¶A NG√ÄY H√îM NAY ƒë·ªÉ tr√°nh duplicate n·∫øu ch·∫°y job nhi·ªÅu l·∫ßn trong ng√†y
    # Gi·ªØ l·∫°i b·∫£n ghi c·ªßa ng√†y h√¥m qua, tu·∫ßn tr∆∞·ªõc...
    today_start = timezone.now().replace(hour=0, minute=0, second=0, microsecond=0)
    
    deleted_count, _ = StudentSnapshot.objects.filter(
        course_id=course_id, 
        created_at__gte=today_start # Ch·ªâ x√≥a c√°i v·ª´a t·∫°o h√¥m nay (n·∫øu ch·∫°y l·∫°i)
    ).delete()
    
    if deleted_count > 0:
        print(f"üîÑ Re-running analytics for today. Deleted {deleted_count} partial records.")

    # Chu·∫©n b·ªã list object
    snapshots = []
    for user_id, row in df_result.iterrows():
        snapshots.append(StudentSnapshot(
            user_id=user_id,
            course_id=course_id,
            
            # C√°c ch·ªâ s·ªë
            engagement_score=row['eng_score'],
            performance_score=row['perf_score'],
            days_inactive=int(row['days_inactive']),
            
            # K·∫øt lu·∫≠n
            risk_level=row['risk_level'],
            ai_message=row['message'],
            
            # [QUAN TR·ªåNG] L∆∞u √Ω: created_at s·∫Ω t·ª± ƒë·ªông l·∫•y gi·ªù hi·ªán t·∫°i (auto_now_add)
        ))
    
    # Bulk Create (Insert 1 c·ª•c)
    StudentSnapshot.objects.bulk_create(snapshots, batch_size=500)
    print(f"‚úÖ Saved history for {len(snapshots)} students in Course {course_id}")

    # [QUAN TR·ªåNG] INVALIDATE CACHE
    # Khi ƒë√£ t√≠nh to√°n xong snapshot m·ªõi, d·ªØ li·ªáu tr√™n Dashboard c≈© ƒë√£ b·ªã l·ªói th·ªùi.
    # Ta x√≥a key cache ƒëi ƒë·ªÉ l·∫ßn t·ªõi Gi·∫£ng vi√™n F5, h·ªá th·ªëng s·∫Ω t√≠nh l·∫°i d·ªØ li·ªáu m·ªõi nh·∫•t.
    cache_key = f"course_pulse_{course_id}"
    cache.delete(cache_key)
    
    print(f"‚ôªÔ∏è Cache invalidated for {cache_key}")


# ==========================================
# GET 
# ==========================================

